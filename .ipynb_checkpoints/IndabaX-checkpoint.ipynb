{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Making in Malaria with AI (beta)\n",
    "\n",
    "In most areas of public policy and decision making, itâ€™s difficult to make evidence based decisions because of the difficulty involved in exploring options for truly complex decisions. Policy making in malaria is a good example of these complex decisions. In this notebook, we explore policy making with malaria by considering only two interventions space i.e. spraying and using bed nets with an open-malaria model for Rachuonyo South in Western Kenya. A reward function based on cost-effectiveness per Daly averted is used. \n",
    "\n",
    "This notebook is structured as follows:\n",
    "\n",
    " 1. **Running the experiments with random actions** : In this section, we run several experiments with random actions and visualize the outcome reward by creating some MatPlotLib visualisations\n",
    " 2. **Implementing Machine Learning and AI on models** : We implement a Genetic Algorithm Model after which we look at reward visualization with MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/slremy/estool\n",
      "  Cloning https://github.com/slremy/estool to /tmp/pip-req-build-g9oizozf\n",
      "Collecting git+https://github.com/slremy/netsapi\n",
      "  Cloning https://github.com/slremy/netsapi to /tmp/pip-req-build-79d7r575\n",
      "Building wheels for collected packages: estool, netsapi\n",
      "  Running setup.py bdist_wheel for estool ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-w2pr5qbt/wheels/ae/d6/49/601d1979c4702fe2f583735c83f276000b8a4d8d0013ca60fd\n",
      "  Running setup.py bdist_wheel for netsapi ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-w2pr5qbt/wheels/9e/73/c9/86a9cc2460e11b3ce5b0a5ebd2d9d332a68afe0941659967fa\n",
      "Successfully built estool netsapi\n",
      "Installing collected packages: estool, netsapi\n",
      "Successfully installed estool-1.0 netsapi-1.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netsapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0e5fb06f8fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetsapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchallenge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetsapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualisation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleGA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRemyGA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'netsapi'"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/slremy/estool git+https://github.com/slremy/netsapi --user --upgrade\n",
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from netsapi.challenge import *\n",
    "from netsapi.visualisation import *\n",
    "from es import SimpleGA, RemyGA\n",
    "\n",
    "from sys import exit, exc_info, argv\n",
    "\n",
    "userId = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Running the experiments with random actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running our first experiment!**\n",
    "\n",
    "Let's start with initializing our experiment to generate our experiment id. Then we run a simulation describing a current intervention campaign 55% ITN and 70% IRS coverage and obtain our reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions = np.array([[0.55, 0.70]])\n",
    "reward = evaluate(actions)\n",
    "solution = np.append(actions.T, reward)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running more experiments with different types of actions**\n",
    "\n",
    "Let's run experiments with 4 sets of actions [ITN, IRS] and visualise the rewards as a simple scatter of points as a response surface (MatPlotLib may be required but you are free to visualise the data how you see fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "actions = np.array([[0.55, 0.70]])\n",
    "reward = evaluate(actions)\n",
    "solution = np.append(actions.T, reward)\n",
    "print(solution)\n",
    "output.append(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([[0.68, 0.70]])\n",
    "reward = evaluate(actions)\n",
    "solution = np.append(actions.T, reward)\n",
    "print(solution)\n",
    "output.append(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([[0.78, 0.70]])\n",
    "reward = evaluate(actions)\n",
    "solution = np.append(actions.T, reward)\n",
    "print(solution)\n",
    "output.append(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([[0.88, 0.70]])\n",
    "reward = evaluate(actions)\n",
    "solution = np.append(actions.T, reward)\n",
    "print(solution)\n",
    "output.append(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions = np.empty(shape=(10,2)) #10 actions [ITN, IRS]\n",
    "actions[:,0] = np.random.rand(10) \n",
    "actions[:,1] = .70               #don't change this yet!\n",
    "rewards = np.array(evaluate(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.hstack((actions,rewards.reshape(-1,1)))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Visualising rewards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResponseSurface(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing Machine Learning and AI on models\n",
    "\n",
    "**Genetic Algorithm**\n",
    "\n",
    "Implemented in ESTool (Ha, David. \"Neuroevolution for deep reinforcement learning problems.\" Proceedings of the Genetic and Evolutionary Computation Conference Companion. ACM, 2018.)\n",
    "\n",
    "\n",
    "We use a parameter size of 2, a population size of 20 and consider 10 generations. For each generation we candidate policies to evaluate based on evolutionary pressure, calculate the reward and create a reward vector. We save all the results in a 3 dimensional vector space and aggregate for all generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_paramters = 2\n",
    "\n",
    "def mutate(chromosome):\n",
    "    mutation_rate = .1\n",
    "    for j in range(num_paramters):\n",
    "        r = np.random.rand(1);\n",
    "        if(r > mutation_rate):\n",
    "            chromosome[j] = np.remainder(chromosome[j]+np.random.randn(1),0.99);\n",
    "    return chromosome\n",
    "\n",
    "def make_random_individuals(x,y):\n",
    "    value=np.random.rand(x,y);\n",
    "    return value\n",
    "\n",
    "\n",
    "def boundary(individual):\n",
    "    processed = np.clip(individual,0,1)\n",
    "    return processed\n",
    "\n",
    "\n",
    "solver = RemyGA(num_paramters,                # number of model parameters\n",
    "            random_individuals_fcn=make_random_individuals,\n",
    "            mutate_fcn = mutate,\n",
    "            sigma_init=1,        # initial standard deviation\n",
    "            popsize=20,   # population size\n",
    "            elite_ratio=0.3,       # percentage of the elites\n",
    "            forget_best=False,     # forget the historical best elites\n",
    "            weight_decay=0.00,     # weight decay coefficient\n",
    "             )\n",
    "\n",
    "num_generations = 10\n",
    "history0=  np.empty(shape=(num_generations, solver.popsize, solver.num_params+1))\n",
    "for i in range(num_generations):\n",
    "    try:\n",
    "        # ask for a set of candidate solutions to be evaluated\n",
    "        solutions = solver.ask(boundary)\n",
    "        # calculate the reward for each given solution using our own method\n",
    "        rewards = evaluate(solutions)\n",
    "        solver.tell(rewards)\n",
    "\n",
    "        # get best parameter, reward from ES\n",
    "        reward_vector = solver.result()\n",
    "        print(reward_vector[0],reward_vector[1],i, num_generations)\n",
    "        history0[i,:,:] = np.hstack((solutions,np.array(rewards).reshape(-1,1)))\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        print(exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of reward vectors**\n",
    "\n",
    "Let's investigate the shape of our result vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reshape our result vector so as to visualize the results for all of the 10 generations in one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResponseSurface(history0.reshape(200,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we are going to visialize the 10 generations in a box plot.\n",
    "\n",
    "Remember, as you \"evolve\" your solutions, you're looking for the best. \n",
    "\n",
    "Do you see any evidence of that in this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot([i[:,-1] for i in history0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now time to try and break it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now free to explore! Given the code snippets of how to post single actions.\n",
    "Several methods, approaches or algorithms may stand to benefit the uses of high bandwidth computation for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "popsize=10\n",
    "num_real_paramters = 2\n",
    "width = 10\n",
    "num_paramters = num_real_paramters * width\n",
    "num_generations = 10\n",
    "\n",
    "#converts from binary representation to the actual action\n",
    "def RealAction(action):\n",
    "    realaction=()\n",
    "    for i in range(int(num_paramters/width)):\n",
    "        a1 = np.array(action[i*width:(i+1)*width])\n",
    "        realaction = realaction + (a1.dot(2**np.arange(a1.size)[::-1])/pow(2,width),)\n",
    "    realaction = np.round(realaction,3);\n",
    "    return realaction\n",
    "\n",
    "def mutate(chromosome):\n",
    "    mutation_rate = .2\n",
    "    for j in range(chromosome.shape[0]):\n",
    "        r = np.random.rand(1);\n",
    "        if(r > mutation_rate):\n",
    "            chromosome[j] = int(np.invert(bool(chromosome[j])));\n",
    "    return chromosome\n",
    "\n",
    "def make_random_individuals(x,y):\n",
    "    value=np.random.choice([0, 1], size=(x,y));\n",
    "    return value\n",
    "\n",
    "def boundary(individual):\n",
    "    processed = individual%(1+np.finfo(float).eps)\n",
    "    return processed\n",
    "\n",
    "solver = RemyGA(num_paramters,         # number of model parameters\n",
    "                random_individuals_fcn=make_random_individuals,\n",
    "                mutate_fcn = mutate,\n",
    "                sigma_init=1,          # initial standard deviation\n",
    "                popsize=popsize,       # population size\n",
    "                elite_ratio=0.3,       # percentage of the elites\n",
    "                forget_best=False,     # forget the historical best elites\n",
    "                weight_decay=0.00,     # weight decay coefficient\n",
    "                )\n",
    "\n",
    "history1=  np.empty(shape=(num_generations, popsize, num_real_paramters+1))\n",
    "for i in range(num_generations):\n",
    "    try:\n",
    "        # ask for a set of random candidate solutions to be evaluated\n",
    "        solutions = solver.ask(boundary)\n",
    "        # calculate the reward for each given solution using our own method\n",
    "        newsolutions = []\n",
    "        for soln in solutions:\n",
    "            newsolutions.append(RealAction(soln))\n",
    "        newsolutions = np.array(newsolutions)\n",
    "        rewards = evaluate(newsolutions)\n",
    "        solver.tell(rewards)\n",
    "        \n",
    "        # get best parameter, reward from ES\n",
    "        reward_vector = solver.result()\n",
    "        print(RealAction(reward_vector[0]), reward_vector[1], i, num_generations)\n",
    "        history1[i,:,:] = np.hstack((newsolutions,np.array(rewards).reshape(-1,1)))\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        print(exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=history1.reshape(num_generations*popsize,num_real_paramters+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResponseSurface(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
